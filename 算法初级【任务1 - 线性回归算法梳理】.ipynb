{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【任务1 - 线性回归算法梳理】时长：2天\n",
    "\n",
    "1 机器学习的一些概念 有监督、无监督、泛化能力、过拟合欠拟合(方差和偏差以及各自解决办法)、交叉验证\n",
    "2 线性回归的原理\n",
    "3 线性回归损失函数、代价函数、目标函数\n",
    "4 优化方法(梯度下降法、牛顿法、拟牛顿法等)\n",
    "5 线性回归的评估指标\n",
    "6 sklearn参数详解\n",
    "学习时长：两天\n",
    "\n",
    "参考：\n",
    "\n",
    "西瓜书\n",
    "\n",
    "cs229吴恩达机器学习课程\n",
    "\n",
    "李航统计学习\n",
    "\n",
    "谷歌搜索\n",
    "\n",
    "公式推导参考：http://t.cn/EJ4F9Q0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 机器学习的一些概念 有监督、无监督、泛化能力、过拟合欠拟合(方差和偏差以及各自解决办法)、交叉验证\n",
    "\n",
    "监督学习：通过已有的一部分输入数据与输出数据之间的对应关系，生成一个函数，将输入映射到合适的输出，例如分类。\n",
    "非监督学习：直接对输入数据集进行建模，例如聚类。\n",
    "半监督学习：综合利用有类标的数据和没有类标的数据，来生成合适的分类函数。\n",
    "\n",
    "泛化能力 泛化能力（指训练好的模型在未见过的数据上的表现）\n",
    "\n",
    "首先欠拟合就是模型没有很好地捕捉到数据特征，不能够很好地拟合数据，\n",
    "解决方法：\n",
    "\n",
    "1）添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。\n",
    "\n",
    "2）添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。例如上面的图片的例子。\n",
    "\n",
    "3）减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。\n",
    "\n",
    "\n",
    "\n",
    "2. 过拟合\n",
    "\n",
    "通俗一点地来说过拟合就是模型把数据学习的太彻底，以至于把噪声数据的特征也学习到了，这样就会导致在后期测试的时候不能够很好地识别数据，即不能正确的分类，模型泛化能力太差。\n",
    "解决方法：\n",
    "\n",
    "1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。\n",
    "\n",
    "2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。\n",
    "\n",
    "3）采用正则化方法。正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对于的范数。但是在机器学习中一般使用L2正则，下面看具体的原因。\n",
    "\n",
    "L0范数是指向量中非0的元素的个数。L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。两者都可以实现稀疏性，既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。\n",
    "\n",
    "L2范数是指向量各元素的平方和然后求平方根。可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。L2正则项起到使得参数w变小加剧的效果，但是为什么可以防止过拟合呢？一个通俗的理解便是：更小的参数值w意味着模型的复杂度更低，对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据，从而使得不会过拟合，以提高模型的泛化能力。还有就是看到有人说L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题（具体这儿我也不是太理解）。\n",
    "\n",
    "4）采用dropout方法。这个方法在神经网络里面很常用。dropout方法是ImageNet中提出的一种方法，通俗一点讲就是dropout方法在训练的时候让神经元以一定的概率不工作。具体看下图：\n",
    "\n",
    "\n",
    "\n",
    "如上图所示，左边a图是没用用dropout方法的标准神经网络，右边b图是在训练过程中使用了dropout方法的神经网络，即在训练时候以一定的概率p来跳过一定的神经元。\n",
    "\n",
    "\n",
    "版权声明：本文为CSDN博主「will_duan」的原创文章，遵循CC 4.0 by-sa版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/willduan1/article/details/53070777\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 线性回归的原理\n",
    "3 线性回归损失函数、代价函数、目标函数\n",
    "4 优化方法(梯度下降法、牛顿法、拟牛顿法等)\n",
    "5 线性回归的评估指标\n",
    "\n",
    "线性回归的原理\n",
    "线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。这样就可以表达特征与结果之间的非线性关系。\n",
    "\n",
    "线性回归损失函数、代价函数、目标函数\n",
    "\n",
    "\n",
    "1.梯度下降法: (1) 先确定向下一步的步伐大小，我们称为学习率α； (2) 任意给定一个初始值:θ； (3) 确定一个向下的方向，并向下走预先规定的步伐，并更新θ值； (4) 当下降的高度小于某个定义的值ε，则停止下降。\n",
    "\n",
    "2.牛顿法: (1) 随机选取起始点x0 (2) 计算目标函数f(x)在该点xk的一阶导数和海森矩阵； (3) 依据迭代公式xk+1=xk−Hk-1f’k更新x值\n",
    "\n",
    "(4) 如果E(f(xk+1)−f(xk))<ϵ，则收敛返回，否则继续步骤2,3直至收敛\n",
    "\n",
    "线性回归的评估指标\n",
    "1.MSE: 均方误差是指参数估计值与参数真值之差平方的期望值。\n",
    "\n",
    "2.RMSE: 均方根误差是均方误差的算术平方根\n",
    "\n",
    "3.MAE: 平均绝对误差是绝对误差的平均值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn参数详解\n",
    "前言\n",
    "随机森林（RandomForest,简称RF）是集成学习bagging的一种代表模型，随机森林模型正如他表面意思，是由若干颗树随机组成一片森林，这里的树就是决策树。\n",
    "\n",
    "在GBDT篇我们说了GBDT和Adaboost的不同，那么RF和GBDT又有什么异同呢？主要有以下两点：\n",
    "\n",
    "模型迭代方式不同，GBDT是boosting模型，RF是bagging模型。\n",
    "GBDT只能使用CART模型，RF默认是CART模型，也可以选择ID3模型。\n",
    "参数同样也分为两部分，一部分是框架提升部分的参数，另一部分是决策树参数。\n",
    "参数\n",
    "\n",
    "class sklearn.ensemble.RandomForestClassifier（\n",
    "    n_estimators=10, criterion='gini', max_depth=None, \n",
    "    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "    max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "    min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, \n",
    "    random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "    \n",
    "因随机森林中的树是决策树，所以关于决策树的大部分参数与前面决策树模型中的参数意思一致，这里就不再赘述，可查看：Sklearn参数详解--决策树\n",
    "n_estimators:随机森林中树的棵树，默认是10棵。\n",
    "criterion:样本集切分策略，默认是gini指数，此时树模型为CART模型，当值选为信息增益的时候，模型就成了ID3模型，默认为CART模型。\n",
    "bootstrap:是统计学中的一种重采样技术，可以简单理解成是有放回地抽样，默认是True,即采取有放回抽样这种策略，这不就是bagging的思想么。\n",
    "oob_score:袋外估计(out-of-bag)，这个外是针对于bagging这个袋子而言的，我们知道，bagging采取的随机抽样的方式去建立树模型，那么那些未被抽取到的样本集，也就是未参与建立树模型的数据集就是袋外数据集，我们就可以用这部分数据集去验证模型效果，默认值为False。\n",
    "\n",
    "对象/属性\n",
    "estimators_：打印输出随机森林中所有的树。\n",
    "\n",
    "\n",
    "classes_：输出样本集的类别。\n",
    "n_classes_：输出类别数量。\n",
    "n_features_：特征数量。\n",
    "n_outputs_：当模型被fit时的输出维度。看看下图来感受一些这个属性。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_importances_：特征重要性。\n",
    "oob_score_：袋外估计准确率得分，必须是oob_score参数选择True的时候才可用。\n",
    "oob_decision_function_：袋外估计对应的决策函数。\n",
    "\n",
    "方法\n",
    "apply(X)：将训练好的模型应用在数据集X上，并返回数据集X对应的叶指数。\n",
    "decision_function(X):返回决策函数值（比如svm中的决策距离）\n",
    "fit(X,Y):在数据集（X,Y）上训练模型。\n",
    "get_parms():获取模型参数\n",
    "predict(X):预测数据集X的结果。\n",
    "predict_log_proba(X):预测数据集X的对数概率。\n",
    "predict_proba(X):预测数据集X的概率值。\n",
    "score(X,Y):输出数据集（X,Y）在模型上的准确率。\n",
    "\n",
    "原文发布于微信公众号 - 张俊红（zhangjunhong0428）\n",
    "\n",
    "原文发表时间：2018-06-30\n",
    "\n",
    "本文参与腾讯云自媒体分享计划，欢迎正在阅读的你也加入，一起分享。\n",
    "\n",
    "发表于 2018-07-30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚类是一种非监督学习，是将一份给定数据集划分成k类，这一份数据集可能是某公司的一批用户，也可能是某媒体网站的一系列文章，如果是某公司的一批用户，那么k-means做的就是根据用户的表现对用户的分类；如果媒体的文章，那么k-means做的就是根据文章的类型，把他分到不同的类别。\n",
    "\n",
    "当一个公司用户发展到一定的量级以后，就没有办法以同样的精力去维护所有用户，这个时候就需要根据用户的种种表现对用户进行分类，然后根据用户类别的不同，采取不同的运营策略。而这里的分类方法就是聚类算法。我们这篇文章主要讲述一下常用的三种聚类方法：\n",
    "\n",
    "K-means聚类\n",
    "层次聚类\n",
    "密度聚类\n",
    "K-means聚类算法\n",
    "K-means聚类算法是最简单、最基础的聚类算法，原理很简单，就是先指定k个点，然后计算每一个样本点分别到这k个点之间的距离，并将不同样本点划分到距离最近的那个点的集合，这样就把所有的样本分成k类了。比如下图就是将所有的样本分为3类。\n",
    "\n",
    "\n",
    "步骤：\n",
    "随机选择K个点（质心）\n",
    "通过计算每个点到这K个点之间的距离（这里的距离默认是欧式距离，一般也选择欧式距离，也可以是其他，比如DTW），并将样本点划分到距离最近的那个点。\n",
    "计算划分后的点的平均值，并将均值作为新的质心，继续进行距离求解，然后重新进行划分，再次求均值，直到均值不发生变化时循环停止。\n",
    "参数\n",
    "class sklearn.cluster.KMeans\n",
    "    (n_clusters=8, init='k-means++', n_init=10, max_iter=300, \n",
    "    tol=0.0001, precompute_distances='auto', verbose=0, \n",
    "    random_state=None, copy_x=True, n_jobs=1, algorithm='auto')\n",
    "n_clusters:质心数量，也就是分类数，默认是8个。\n",
    "\n",
    "init:初始化质心的选取方式，主要有下面三种参数可选，‘k-means++’、‘random’ or an ndarray，默认是'k-means++'。因为初始质心是随机选取的，会造成局部最优解，所以需要更换几次随机质心，这个方法在sklearn中通过给init参数传入=“k-means++”即可。\n",
    "\n",
    "K-means与K-means++区别：\n",
    "原始K-means算法最开始随机选取数据集中K个点作为聚类中心，而K-means++按照如下的思想选取K个聚类中心：\n",
    "假设已经选取了n个初始聚类中心(0<n<K)，则在选取第n+1个聚类中心时：距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心，但在选取第一个聚类中心(n=1)时同样通过随机的方法，之所以这样做是因为聚类中心互相离得越远越好。\n",
    "\n",
    "n_init:随机初始化的次数，kmeans质心迭代的次数。\n",
    "max_iter:最大迭代次数，默认是300。\n",
    "tol:误差容忍度最小值。\n",
    "precompute_distances:是否需要提前计算距离，auto,True,False三个参数值可选。默认值是auto，如果选择auto，当样本数*质心数>12兆的时候，就不会提前进行计算，如果小于则会与提前计算。提前计算距离会让聚类速度很快，但是也会消耗很多内存。\n",
    "copy_x:主要起作用于提前计算距离的情况，默认值是True,如果是True,则表示在源数据的副本上提前计算距离时，不会修改源数据。\n",
    "algorithm：优化算法的选择，有auto、full和elkan三种选择。full就是一般意义上的K-Means算法，elkan是使用的elkan K-Means算法。默认的auto则会根据数据值是否是稀疏的(稀疏一般指是有大量缺失值)，来决定如何选择full和elkan。如果数据是稠密的，就选择elkan K-means，否则就使用普通的Kmeans算法。\n",
    "\n",
    "\n",
    "刘建平大佬博客关于elkan算法的解释\n",
    "\n",
    "对象/属性\n",
    "cluster_centers_:输出聚类的质心。\n",
    "labels_:输出每个样本集对应的类别。\n",
    "inertia_:所有样本点到其最近点距离之和。\n",
    "\n",
    "层次聚类\n",
    "层次聚类有两种方式，一种是从上至下（凝聚法），另一种是从下至上（分裂法），如下图所示。\n",
    "\n",
    "\n",
    "从下至上（凝聚法）\n",
    "从上至下就是把每一个样本分别当作一类，然后计算两两样本之间的距离，将距离较近的两个样本进行合并，再计算两两合并以后的簇之间的距离，将距离最近的两个簇进行合并，重复执行这个过程，直到达到最后指定的类别数或者达到了停止条件。\n",
    "\n",
    "从上至下（分裂法）\n",
    "从下至上就是刚开始把所有样本都当作同一类，然后计算两两样本之间的距离，将距离较远的两个样本分割成两类，然后再计算剩余样本集中每个样本到这两类中的距离，距离哪类比较近，则把样本划分到哪一类，循环执行这个过程，直到达到最后指定的类别数或者达到了停止条件。\n",
    "\n",
    "参数\n",
    "AgglomerativeClustering是用来实现凝聚法聚类模型的。\n",
    "\n",
    "class sklearn.cluster.AgglomerativeClustering\n",
    "    (n_clusters=2, affinity='euclidean', memory=None, \n",
    "    connectivity=None, compute_full_tree='auto', linkage='ward', \n",
    "    pooling_func=<function mean at 0x174b938>)\n",
    "n_clusters：目标类别数，默认是2。\n",
    "affinity：样本点之间距离计算方式，可以是euclidean(欧式距离), l1、 l2、manhattan(曼哈顿距离)、cosine(余弦距离)、precomputed(可以预先设定好距离)，如果参数linkage选择“ward”的时候只能使用euclidean。\n",
    "linkage：链接标准，即样本点的合并标准，主要有ward、complete、average三个参数可选，默认是ward。每个簇（类）本身就是一个集合，我们在合并两个簇的时候其实是在合并两个集合，所以我们需要找到一种计算两个集合之间距离的方式，主要有这三种方式：ward、complete、average，分别表示使用两个集合方差、两个集合中点与点距离之间的平均值、两个集合中距离最小的两个点的距离。\n",
    "\n",
    "对象/属性\n",
    "labels_:每个样本点的类别。\n",
    "n_leaves_:层次树中叶结点树。\n",
    "\n",
    "密度聚类：\n",
    "密度聚类与前面两种聚类方式不同，密度聚类无法事先指定类别个数，只能通过去指定每个点的邻域，以及邻域内包含样本点的最少个数。\n",
    "\n",
    "先来看几个密度聚类里面用到的概念：\n",
    "\n",
    "邻域：邻域是针对样本集中的每个点而言的，我们把距离某个样本点（可以把该点理解为圆心）距离在r（可理解为圆的半径）以内的点的集合称为该点的邻域。\n",
    "核心对象：如果某个点的邻域内至少包含MinPts个样本，则该点就可以称为核心对象。\n",
    "密度直达：如果点A位于点B的邻域中，且点B是核心对象，则称A和B是密度直达。\n",
    "密度可达：对于点A和B，如果存在一个（或者若干个）点C，其中A到C是密度直达，C到B是密度直达，则A和B就称为密度可达。（你可以理解为C是一个跳板，你可以通过C从点A跳到B）\n",
    "密度相连：若存在一个点C，使得C到A是密度直达，C到B是密度直达，则称A和B是密度相连的。\n",
    "具体步骤\n",
    "先建立几个集合，一个用来存储核心对象的集合Ω，初始值是空集；再初始化一个值k，用来存放簇的类别数，初始值为0；再新建一个集合Γ，用来存放未被使用的样本，初始值为全部样本集D。\n",
    "遍历所有样本集中的每个样本点p，判断其是否满足核心对象的条件，如果满足，则把该点加入到核心对象集合Ω中；如果没有样本点满足核心对象条件，则结束遍历。\n",
    "判断核心对象集合Ω是否为空，如果为空，则算法结束；如果不为空，则在集合Ω中随机选取一个样本点，将该点密度可达的所有样本点划分为一个簇，这个簇的样本集合称为Ck ，将簇的类别数k+1，未被使用样本Γ-Ck。\n",
    "4.再在剩余的核心对象中重复步骤3，直到没有核心对象为止。\n",
    "5.最后输出k个类别的样本集合{C1、C2、……、Ck}。\n",
    "参数\n",
    "class sklearn.cluster.DBSCAN\n",
    "    (eps=0.5, min_samples=5, metric='euclidean', metric_params=None, \n",
    "    algorithm='auto', leaf_size=30, p=None, n_jobs=1)\n",
    "eps:即邻域中的r值，可以理解为圆的半径。\n",
    "min_samples:要成为核心对象的必要条件，即邻域内的最小样本数，默认是5个。\n",
    "metric:距离计算方式，和层次聚类中的affinity参数类似，同样也可以是precomputed。\n",
    "metric_params:其他度量函数的参数。\n",
    "algorithm:最近邻搜索算法参数，auto、ball_tree(球树)、kd_tree（kd树）、brute(暴力搜索)，默认是auto。\n",
    "leaf_size:最近邻搜索算法参数，当algorithm使用kd_tree或者ball_tree时，停止建子树的叶子节点数量的阈值。\n",
    "p: 最近邻距离度量参数。只用于闵可夫斯基距离和带权重闵可夫斯基距离中p值的选择，p=1为曼哈顿距离，p=2为欧式距离。\n",
    "\n",
    "对象/属性\n",
    "core_sample_indices_：核心对象数。\n",
    "labels_：每个样本点的对应的类别，对于噪声点将赋值为-1。\n",
    "\n",
    "参考文章：\n",
    "\n",
    "http://www.cnblogs.com/pinard/p/6217852.html\n",
    "\n",
    "原文发布于微信公众号 - 张俊红（zhangjunhong0428）\n",
    "\n",
    "原文发表时间：2018-07-07\n",
    "\n",
    "本文参与腾讯云自媒体分享计划，欢迎正在阅读的你也加入，一起分享。\n",
    "\n",
    "发表于 2018-07-30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
