{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集体智能，很多个模型一起投票决定，大家一起做决定\n",
    "\n",
    "任务内容：\n",
    "\n",
    "随机森林算法梳理\n",
    "\n",
    "集成学习的概念\n",
    "个体学习器的概念\n",
    "boosting bagging的概念、异同点\n",
    "理解不同的结合策略(平均法，投票法，学习法)\n",
    "随机森林的思想\n",
    "随机森林的推广\n",
    "随机森林的优缺点\n",
    "随机森林在sklearn中的参数解释\n",
    "随机森林的应用场景\n",
    "参考：\n",
    "\n",
    "西瓜书\n",
    "\n",
    "cs229吴恩达机器学习课程\n",
    "\n",
    "李航统计学习\n",
    "\n",
    "谷歌搜索\n",
    "\n",
    "公式推导参考：http://t.cn/EJ4F9Q0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前言\n",
    "随机森林（RandomForest,简称RF）是集成学习bagging的一种代表模型，随机森林模型正如他表面意思，是由若干颗树随机组成一片森林，这里的树就是决策树。\n",
    "\n",
    "在GBDT篇我们说了GBDT和Adaboost的不同，那么RF和GBDT又有什么异同呢？主要有以下两点：\n",
    "\n",
    "模型迭代方式不同，GBDT是boosting模型，RF是bagging模型。\n",
    "GBDT只能使用CART模型，RF默认是CART模型，也可以选择ID3模型。\n",
    "参数同样也分为两部分，一部分是框架提升部分的参数，另一部分是决策树参数。\n",
    "参数\n",
    "class sklearn.ensemble.RandomForestClassifier（\n",
    "    n_estimators=10, criterion='gini', max_depth=None, \n",
    "    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "    max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "    min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, \n",
    "    random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "因随机森林中的树是决策树，所以关于决策树的大部分参数与前面决策树模型中的参数意思一致，这里就不再赘述，可查看：Sklearn参数详解--决策树\n",
    "n_estimators:随机森林中树的棵树，默认是10棵。\n",
    "criterion:样本集切分策略，默认是gini指数，此时树模型为CART模型，当值选为信息增益的时候，模型就成了ID3模型，默认为CART模型。\n",
    "bootstrap:是统计学中的一种重采样技术，可以简单理解成是有放回地抽样，默认是True,即采取有放回抽样这种策略，这不就是bagging的思想么。\n",
    "oob_score:袋外估计(out-of-bag)，这个外是针对于bagging这个袋子而言的，我们知道，bagging采取的随机抽样的方式去建立树模型，那么那些未被抽取到的样本集，也就是未参与建立树模型的数据集就是袋外数据集，我们就可以用这部分数据集去验证模型效果，默认值为False。\n",
    "\n",
    "对象/属性\n",
    "estimators_：打印输出随机森林中所有的树。\n",
    "\n",
    "\n",
    "\n",
    "classes_：输出样本集的类别。\n",
    "n_classes_：输出类别数量。\n",
    "n_features_：特征数量。\n",
    "n_outputs_：当模型被fit时的输出维度。看看下图来感受一些这个属性。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_importances_：特征重要性。\n",
    "oob_score_：袋外估计准确率得分，必须是oob_score参数选择True的时候才可用。\n",
    "oob_decision_function_：袋外估计对应的决策函数。\n",
    "\n",
    "方法\n",
    "apply(X)：将训练好的模型应用在数据集X上，并返回数据集X对应的叶指数。\n",
    "decision_function(X):返回决策函数值（比如svm中的决策距离）\n",
    "fit(X,Y):在数据集（X,Y）上训练模型。\n",
    "get_parms():获取模型参数\n",
    "predict(X):预测数据集X的结果。\n",
    "predict_log_proba(X):预测数据集X的对数概率。\n",
    "predict_proba(X):预测数据集X的概率值。\n",
    "score(X,Y):输出数据集（X,Y）在模型上的准确率。\n",
    "\n",
    "原文发布于微信公众号 - 张俊红（zhangjunhong0428）\n",
    "\n",
    "原文发表时间：2018-06-30\n",
    "\n",
    "本文参与腾讯云自媒体分享计划，欢迎正在阅读的你也加入，一起分享。\n",
    "\n",
    "发表于 2018-07-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成学习的概念\n",
    "https://www.cnblogs.com/GuoJiaSheng/p/3865684.html\n",
    "步骤：\n",
    "\n",
    "  1）通过不同方法训练出多个学习器（比如使用同质的分类器不同的训练数据和参数设置，使用不一样的分类器。。。。）\n",
    "\n",
    "  2）集成多个分类器的结果。\n",
    "\n",
    "      在集成学习系统中，分量学习器的输出形式对采用何种集成方法有很大的影响，根据输出形式我们可以这么分：\n",
    "\n",
    "     （1）基于抽象级信息的集成\n",
    "\n",
    "       所谓抽象指的是分量分类器直接输出单纯的识别结果，没有其他附加信息，是最普遍的一种方法，比如常用的投票，加权投票等。\n",
    "\n",
    "     （2）基于排序级（目前我还不知是啥意思）\n",
    "\n",
    "     （3）基于度量级信息的集成\n",
    "\n",
    "      指的是分量分类器输出为度量值，如概率，信度等。比如Max、Min、Sum。Product、Median等等（weka里面有）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "个体学习器的概念\n",
    "https://hyper.ai/wiki/4285\n",
    "个体学习器是一个相对概念，在  集成学习 中，集成之前的学习器称为个体学习器。\n",
    "\n",
    "目前集成学习的方法大致可分为两大类：\n",
    "\n",
    "个体学习器间存在强依赖关系、必须串行生成的序列化方法，代表是 Boosting；\n",
    "个体学习器间不存在强依赖关系、可同时生成的并行化方法，代表是 Baggig 和 “随机森林”（Random Forest）；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boosting bagging的概念、异同点\n",
    "https://www.cnblogs.com/liuwu265/p/4690486.html\n",
    "    \n",
    "Bagging和Boosting 概念及区别\n",
    "　　Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。\n",
    "\n",
    "首先介绍Bootstraping，即自助法：它是一种有放回的抽样方法（可能抽到重复的样本）。\n",
    "\n",
    "1、Bagging (bootstrap aggregating)\n",
    "\n",
    "Bagging即套袋法，其算法过程如下：\n",
    "\n",
    "A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）\n",
    "\n",
    "B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）\n",
    "\n",
    "C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）\n",
    "\n",
    " \n",
    "\n",
    "2、Boosting\n",
    "\n",
    "其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。\n",
    "\n",
    "关于Boosting的两个核心问题：\n",
    "\n",
    "1）在每一轮如何改变训练数据的权值或概率分布？\n",
    "\n",
    "通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。\n",
    "\n",
    "2）通过什么方式来组合弱分类器？\n",
    "\n",
    "通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。\n",
    "\n",
    "而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。\n",
    "\n",
    " \n",
    "\n",
    "3、Bagging，Boosting二者之间的区别\n",
    "\n",
    "Bagging和Boosting的区别：\n",
    "\n",
    "1）样本选择上：\n",
    "\n",
    "Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。\n",
    "\n",
    "Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。\n",
    "\n",
    "2）样例权重：\n",
    "\n",
    "Bagging：使用均匀取样，每个样例的权重相等\n",
    "\n",
    "Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。\n",
    "\n",
    "3）预测函数：\n",
    "\n",
    "Bagging：所有预测函数的权重相等。\n",
    "\n",
    "Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。\n",
    "\n",
    "4）并行计算：\n",
    "\n",
    "Bagging：各个预测函数可以并行生成\n",
    "\n",
    "Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。\n",
    "\n",
    " \n",
    "\n",
    "4、总结\n",
    "\n",
    "这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。\n",
    "\n",
    "下面是将决策树与这些算法框架进行结合所得到的新的算法：\n",
    "\n",
    "1）Bagging + 决策树 = 随机森林\n",
    "\n",
    "2）AdaBoost + 决策树 = 提升树\n",
    "\n",
    "3）Gradient Boosting + 决策树 = GBDT\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "参考文献\n",
    "\n",
    "[1] 林轩田，机器学习技法。\n",
    "\n",
    "[2] IRLAB, http://www.cnblogs.com/guolei/archive/2013/05/21/3091301.html\n",
    "\n",
    "[3] 百度技术，http://baidutech.blog.51cto.com/4114344/743809/\n",
    "理解不同的结合策略(平均法，投票法，学习法)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林的思想\n",
    "随机森林的推广\n",
    "随机森林的优缺点\n",
    "随机森林在sklearn中的参数解释\n",
    "随机森林的应用场景\n",
    "https://zhuanlan.zhihu.com/p/58064259\n",
    "    \n",
    "5. 随机森林思想\n",
    "\n",
    "随机森林的思路很简单如下：\n",
    "\n",
    "从样本集中用Bootstrap采样选出n个样本;\n",
    "\n",
    "从所有属性中随机选择K个属性，选择出最佳分割属性作为节点创建决策树\n",
    "\n",
    "重复以上两步m次，即建立m棵决策树\n",
    "\n",
    "这m个决策树形成随机森林，通过投票表决结果决定数据属于那一类\n",
    "\n",
    "\n",
    "\n",
    "6.随机森林的推广\n",
    "\n",
    "由于RF在实际应用中的良好特性，基于RF，有很多变种算法，应用也很广泛，不光可以用于分类回归，还可以用于特征转换，异常点检测等。下面对于这些RF家族的算法中有代表性的做一个总结。\n",
    "\n",
    "6.1 extra trees\n",
    "\n",
    "extra trees是RF的一个变种, 原理几乎和RF一模一样，仅有区别有：\n",
    "1）对于每个决策树的训练集，RF采用的是随机采样bootstrap来选择采样集作为每个决策树的训练集，而extra trees一般不采用随机采样，即每个决策树采用原始训练集。\n",
    "2）在选定了划分特征后，RF的决策树会基于信息增益，基尼系数，均方差之类的原则，选择一个最优的特征值划分点，这和传统的决策树相同。但是extra trees比较的激进，他会随机的选择一个特征值来划分决策树。\n",
    "从第二点可以看出，由于随机选择了特征值的划分点位，而不是最优点位，这样会导致生成的决策树的规模一般会大于RF所生成的决策树。也就是说，模型的方差相对于RF进一步减少，但是偏倚相对于RF进一步增大。在某些时候，extra trees的泛化能力比RF更好。\n",
    "\n",
    "6.2 Totally Random Trees Embedding\n",
    "\n",
    "Totally Random Trees Embedding(以下简称 TRTE)是一种非监督学习的数据转化方法。它将低维的数据集映射到高维，从而让映射到高维的数据更好的运用于分类回归模型。我们知道，在支持向量机中运用了核方法来将低维的数据集映射到高维，此处TRTE提供了另外一种方法。\n",
    "TRTE在数据转化的过程也使用了类似于RF的方法，建立T个决策树来拟合数据。当决策树建立完毕以后，数据集里的每个数据在T个决策树中叶子节点的位置也定下来了。比如我们有3颗决策树，每个决策树有5个叶子节点，某个数据特征xx划分到第一个决策树的第2个叶子节点，第二个决策树的第3个叶子节点，第三个决策树的第5个叶子节点。则x映射后的特征编码为(0,1,0,0,0, 0,0,1,0,0, 0,0,0,0,1), 有15维的高维特征。这里特征维度之间加上空格是为了强调三颗决策树各自的子编码。\n",
    "映射到高维特征后，可以继续使用监督学习的各种分类回归算法了。\n",
    "\n",
    "6.3 Isolation Forest\n",
    "\n",
    "这个算法是用来异常点检测的，正如isolation这个名字，是找出非正常的点，而这些非正常的点显然是特征比较明确的，故不需要太多的数据，也不需要太大规模的决策树。\n",
    "\n",
    "它和RF算法有以下几个差别：\n",
    "\n",
    "在随机采样的过程中，一般只需要少量数据即可。\n",
    "\n",
    "在进行决策树构建过程中，IForest算法会随机选择一个划分特征，并对划分特征随机选择一个划分阈值。\n",
    "\n",
    "IForest算法构建的决策树一般深度max_depth是比较小的。\n",
    "\n",
    "\n",
    "\n",
    "7.优缺点\n",
    "\n",
    "RF的主要优点：\n",
    "\n",
    "训练可以并行化，对于大规模样本的训练具有速度的优势。\n",
    "\n",
    "由于进行随机选择决策树划分特征列表，这样在样本维度比较高的时候，仍然具有比较高的训练性能。\n",
    "\n",
    "可以给出各个特征的重要性列表。\n",
    "\n",
    "由于存在随机抽样，训练出来的模型方差小，泛化能力强。\n",
    "\n",
    "RF实现简单。\n",
    "\n",
    "对于部分特征的缺失不敏感。\n",
    "\n",
    "\n",
    "\n",
    "RF的主要缺点：\n",
    "\n",
    "在某些噪音比较大的特征上，RF模型容易陷入过拟合。\n",
    "\n",
    "取值比较多的划分特征对RF的决策会产生更大的影响，从而有可能影响模型的效果。\n",
    "\n",
    "随机森林在解决回归问题时并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续型的输出。当进行回归时，随机森林不能够作出超越训练集数据范围的预测，这可能导致在对某些还有特定噪声的数据进行建模时出现过度拟合。\n",
    "\n",
    "对于许多统计建模者来说，随机森林给人的感觉像是一个黑盒子——你几乎无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试。\n",
    "\n",
    "\n",
    "\n",
    "8.sklearn参数\n",
    "\n",
    "请参考下文：\n",
    "\n",
    "http://www.cnblogs.com/jasonfreak/p/5720137.html\n",
    "\n",
    "\n",
    "\n",
    "9.应用场景\n",
    "\n",
    "RF算法在实际应用中具有比较好的特性，应用也比较广泛，主要应用在分类、 回归、特征转换、特征重要度挑选、异常点检测等。\n",
    "\n",
    "随机森林在现实分析中被大量使用，它相对于决策树，在准确性上有了很大的提升，同时一定程度上改善了决策树容易被攻击的特点。\n",
    "\n",
    "数据维度相对低（几十维），同时对准确性有较高要求时。\n",
    "\n",
    "因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
