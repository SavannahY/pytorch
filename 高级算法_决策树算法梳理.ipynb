{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【任务3 - 决策树算法梳理】时长：2天\n",
    "\n",
    "信息论基础（熵 联合熵 条件熵 信息增益 基尼不纯度） 2.决策树的不同分类算法（ID3算法、C4.5、CART分类树）的原理及应用场景\n",
    "回归树原理\n",
    "决策树防止过拟合手段\n",
    "模型评估\n",
    "sklearn参数详解，Python绘制决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1信息论基础（熵 联合熵 条件熵 信息增益 基尼不纯度）\n",
    "（1）熵： 表示随机变量不确定性的度量。\n",
    "\n",
    "（2）联合熵 ：一组变量之间不确定性的衡量手段。\n",
    "两个变量\n",
    "\n",
    "多个变量\n",
    "\n",
    "\n",
    "（3）条件熵 ：表示在已知随机变量X的条件下随机变量Y的不确定性。\n",
    "\n",
    "（4）信息增益 ：表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。\n",
    "（5）基尼不纯度：是指将来自集合中的某种结果随机应用在集合中，某一数据项的预期误差率。在进行决策树编程的时候，可以作为衡量系统混乱程度的标准。\n",
    "--------------------- \n",
    "版权声明：本文为CSDN博主「言成苟文」的原创文章，遵循CC 4.0 by-sa版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/weixin_43760925/article/details/98943817"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 决策树的不同分类算法（ID3算法、C4.5、CART分类树）的原理及应用场景\n",
    "ID3：每次选择信息增益最大的特征进行划分\n",
    "C4.5：每次选择信息增益比最大的特征进行划分\n",
    "CART：每次选择基尼不纯度减少最多的特征以及特征值进行划分\n",
    "\n",
    "ID3算法\n",
    "在ID3算法中，选择信息增益最大的属性作为当前的特征对数据集分类，通过不断的选择特征对数据集不断划分。但只能处理离散型属性。\n",
    "\n",
    "C4.5\n",
    "C4.5算法流程与ID3相类似，只不过将信息增益改为信息增益比，以解决偏向取值较多的属性的问题，另外它可以处理连续型属性。\n",
    "\n",
    "CART分类树\n",
    "CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。相对于ID3使用的信息增益，CART中用于选择变量的不纯性度量是Gini指数，总体内包含的类别越杂乱，GINI指数就越大（跟熵的概念很相似）。\n",
    "参考 https://blog.csdn.net/github_39261590/article/details/76546281\n",
    "--------------------- \n",
    "版权声明：本文为CSDN博主「Karin Su」的原创文章，遵循CC 4.0 by-sa版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/Karinsu/article/details/99166618"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3回归树原理\n",
    "回归树使用最大均方差划分节点；每个节点样本的均值作为测试样本的回归预测值。\n",
    "一、概念\n",
    "CART全称叫Classification and Regression Tree。首先要强调的是CART假设决策树是二叉树，内部结点特征的取值只有“是”和“否”，左分支是取值为“是”的分支，有分支则相反。这样的决策树等价于递归地二分每个特征。\n",
    "二、CART生成\n",
    "决策树的生成就是递归地构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4决策树防止过拟合手段\n",
    "剪枝是决策树学习算法对付“过拟合”的主要手段，在决策树学习中，为了尽可能正确分类训练样本，结点划分过程不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得“太好”了，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合，因此，可通过主动去掉一些分支来降低过拟合的风险。\n",
    "\n",
    "        决策树剪枝的基本策略有“预剪枝”和“后剪枝”。预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能力提升，则停止划分并将当前结点标记为叶结点；后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。\n",
    "\n",
    "引用西瓜书上的例子理解“预剪枝”和“后剪枝”\n",
    "--------------------- \n",
    "版权声明：本文为CSDN博主「Karin Su」的原创文章，遵循CC 4.0 by-sa版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/Karinsu/article/details/99166618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5模型评估\n",
    "1、随机二次抽样\n",
    "2、交叉验证\n",
    "3、自助法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 sklearn参数详解，Python绘制决策树\n",
    "criterion:特征选择的标准，有信息增益和基尼系数两种，使用信息增益的是ID3和C4.5算法（使用信息增益比），使用基尼系数的CART算法，默认是gini系数。\n",
    "\n",
    "splitter:特征切分点选择标准，决策树是递归地选择最优切分点，spliter是用来指明在哪个集合上来递归，有“best”和“random”两种参数可以选择，best表示在所有特征上递归，适用于数据集较小的时候，random表示随机选择一部分特征进行递归，适用于数据集较大的时候。\n",
    "\n",
    "max_depth:决策树最大深度，决策树模型先对所有数据集进行切分，再在子数据集上继续循环这个切分过程，max_depth可以理解成用来限制这个循环次数。\n",
    "\n",
    "min_samples_split:子数据集再切分需要的最小样本量，默认是2，如果子数据样本量小于2时，则不再进行下一步切分。如果数据量较小，使用默认值就可，如果数据量较大，为降低计算量，应该把这个值增大，即限制子数据集的切分次数。\n",
    "\n",
    "min_samples_leaf:叶节点（子数据集）最小样本数，如果子数据集中的样本数小于这个值，那么该叶节点和其兄弟节点都会被剪枝（去掉），该值默认为1。\n",
    "\n",
    "min_weight_fraction_leaf:在叶节点处的所有输入样本权重总和的最小加权分数，如果不输入则表示所有的叶节点的权重是一致的。\n",
    "\n",
    "max_features:特征切分时考虑的最大特征数量，默认是对所有特征进行切分，也可以传入int类型的值，表示具体的特征个数；也可以是浮点数，则表示特征个数的百分比；还可以是sqrt,表示总特征数的平方根；也可以是log2，表示总特征数的log个特征。\n",
    "\n",
    "random_state:随机种子的设置，与LR中参数一致。\n",
    "\n",
    "max_leaf_nodes:最大叶节点个数，即数据集切分成子数据集的最大个数。\n",
    "\n",
    "min_impurity_decrease:切分点不纯度最小减少程度，如果某个结点的不纯度减少小于这个值，那么该切分点就会被移除。\n",
    "\n",
    "min_impurity_split:切分点最小不纯度，用来限制数据集的继续切分（决策树的生成），如果某个节点的不纯度（可以理解为分类错误率）小于这个阈值，那么该点的数据将不再进行切分。\n",
    "\n",
    "class_weight:权重设置，主要是用于处理不平衡样本，与LR模型中的参数一致，可以自定义类别权重，也可以直接使用balanced参数值进行不平衡样本处理。\n",
    "\n",
    "presort:是否进行预排序，默认是False，所谓预排序就是提前对特征进行排序，我们知道，决策树分割数据集的依据是，优先按照信息增益/基尼系数大的特征来进行分割的，涉及的大小就需要比较，如果不进行预排序，则会在每次分割的时候需要重新把所有特征进行计算比较一次，如果进行了预排序以后，则每次分割的时候，只需要拿排名靠前的特征就可以了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
